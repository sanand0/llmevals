<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LLM Mental Math</title>
  <link
    rel="icon"
    type="image/svg+xml"
    href="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAzMiAzMiI+PGNpcmNsZSBjeD0iMTYiIGN5PSIxNiIgcj0iMTUiIGZpbGw9IiMyNTYzZWIiLz48cGF0aCBmaWxsPSIjZmZmIiBkPSJtMTYgNyAyIDcgNyAyLTcgMi0yIDctMi03LTctMiA3LTJaIi8+PC9zdmc+" />
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    .narrative {
      max-width: 40rem;
    }

    .popover-body {
      white-space: pre-wrap;
      font-family: var(--bs-font-monospace);
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
</head>

<body>
  <div class="container pt-5">
    <h1 class="display-1 my-4 text-center">Dealing with Hallucinations</h1>
    <h2 class="display-6 text-center">How can we rely on unreliable LLMs?</h2>
  </div>

  <div class="container-fluid">
    <div class="fs-5 p-3 mx-auto narrative">
      <p>Large language models can feel magical—until they start confidently making things up. In a recent evaluation of 11 state-of-the-art LLMs on a real-world customer-support intent task, we discovered a simple way to tame those “hallucinations” with only a modest hit to automation.</p>
      <p>We tested 11 LLMs on Kaggle’s <a href="https://www.kaggle.com/datasets/scodepy/customer-support-intent-dataset">Customer Support Intent Dataset</a>, asking them to classify user messages (billing, refunds, order changes, etc.).</p>
    </div>
  </div>

  <div class="table-responsive">
    <table class="table table-hover table-bordered">
      <thead class="table-dark">
        <tr id="headerRow"></tr>
      </thead>
      <tbody id="body"></tbody>
      <tfoot id="foot"></tfoot>
    </table>
  </div>

  <div class="fs-5 p-3 mx-auto narrative">
    <p>The headline findings:</p>
    <ul>
      <li><strong>Median accuracy ~90%.</strong> Over half the models got at least 90% of the examples right—so even a random pick would nail most queries.</li>
      <li><strong>Error rate ~14%.</strong> But in scale-sensitive settings (customer support, compliance, etc.) a 1 in 7 mistake rate can be expensive.</li>
    </ul>
  </div>

  <div class="container text-center">
    <img class="img-fluid" src="model-accuracy.webp" alt="Graph of classification accuracy. Only 20% of the customer intents are classified with under 80% accuracy">
  </div>

  <div class="fs-5 my-5 p-3 mx-auto narrative">
    <p>Different LLMs tend to trip over different edge cases. One might misinterpret “cancel my order”; another might choke on a subtle refund clause. That diversity of mistakes is actually our superpower.</p>

    <h2 id="crosscheck-with-other-models">Cross‐check with other models</h2>
    <p>Imagine running each query through two independent LLMs and flagging any time they disagree. The correlation between LLM models is fairly low, as shown below:</p>
  </div>

  <div class="container text-center">
    <img class="img-fluid" src="correlation.webp" alt="Correlation between LLMs. Numbers are typically between 10-30%">
  </div>

  <div class="fs-5 my-5 p-3 mx-auto narrative">
    <p>So, if you asked two or more models the same question, we can pass any disagreements to a human to cross-check. This adds effort but improves quality. By how much?</p>
    <table class="table table-striped">
      <thead>
        <tr>
          <th style="text-align: right;">Models</th>
          <th style="text-align: right;">Effort</th>
          <th style="text-align: right;">Error</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: right;">1</td>
          <td style="text-align: right;">0%</td>
          <td style="text-align: right;">14.1%</td>
        </tr>
        <tr>
          <td style="text-align: right;">2</td>
          <td style="text-align: right;">12.6%</td>
          <td style="text-align: right;">3.7%</td>
        </tr>
        <tr>
          <td style="text-align: right;">3</td>
          <td style="text-align: right;">18.5%</td>
          <td style="text-align: right;">2.2%</td>
        </tr>
        <tr>
          <td style="text-align: right;">4</td>
          <td style="text-align: right;">23.0%</td>
          <td style="text-align: right;">1.5%</td>
        </tr>
        <tr>
          <td style="text-align: right;">5</td>
          <td style="text-align: right;">28.1%</td>
          <td style="text-align: right;">0.7%</td>
        </tr>
      </tbody>
    </table>
  </div>

  <div class="container">
    <p><img class="img-fluid" alt="As models increase, effort increases roughly linearly and error tapers to zero" src="improvement.webp"></p>
  </div>

  <div class="fs-5 my-5 p-3 mx-auto narrative">
    <p>That is, in this case:</p>
    <ul>
      <li>If you double-check with 2 models, they typically disagree ~13% of the time, so automation only saves 87% effort. But errors <strong>drop from 14% to ~4%</strong>!</li>
      <li>If you <em>triple</em>-check, automation saves only ~81% effort with errors improving to ~2%</li>
      <li>If you <em>quadruple</em>-check, saving is ~77% with errors of ~1.5%</li>
      <li>If you <em>quintuple</em>-check, saving is ~72% with errors under 1%</li>
    </ul>
    <p>Why does this work? Because the <strong>mistakes are <em>mostly</em> independent.</strong> LLMs trained on different data and architectures err in uncorrelated ways. Two models wrongly picking the <em>same</em> wrong answer from multiple choices is low.</p>
    <p><strong>WARNING</strong>: Drop models that are consistently bad. In this case, we dropped <code>google/gemma-3-27b-it</code>.</p>
    <p>So, we can introduce <strong>human-in-the-loop only when needed.</strong> Reviewers only see the ~20–25% cases where models disagree, saving time and catching nearly every error.</p>
    <p>The choice of model combinations could matter. For example, pairing <code>amazon/nova-lite-v1</code> with <code>openai/gpt-4.1-mini</code> adds only <strong>~10% effort for a 2% error rate</strong>. The best combination would depend on your use case.</p>
    <h2 id="takeaway">Takeaway</h2>
    <p>If you’re battling hallucinations in your LLM pipeline, you don’t need a perfect single model—just <strong>double-check</strong>:</p>
    <blockquote class="blockquote">
      <p><strong>For a ~25% review load, multi-modal checks boost accuracy from ~85% to ~99%.</strong></p>
    </blockquote>
    <p>In short, <strong>rely on multiple (unreliable) LLMs</strong> rather than a single imperfect one. By turning your model fleet into a built-in safety net, you can automate at speed <strong>and</strong> quality—finally making hallucination a thing of the past.</p>
    <p>Oh, it also saves 25% of the jobs, but you’ll need to train them on reviewing.</p>
  </div>

  <script type="module">
    const { config, results } = await(await fetch('evals.json')).json();
    let tests = [...new Set(results.results.map(t => t.vars.query))];
    const entries = results.results;
    const providers = [...new Set(entries.map(r => r.provider.id || r.provider))];
    const stats = providers.map(m => {
      const res = { model: m, pass: 0, fail: 0, error: 0, blank: 0 };
      tests.forEach((d, i) => {
        const r = entries.find(r => (r.provider.id || r.provider) === m && r.vars.query === d);
        const o = r.response;
        if (o?.error) res.error++;
        else if (!(o?.output || '').trim()) res.blank++;
        else if (r.success) res.pass++;
        else res.fail++;
      });
      const den = res.pass + res.fail;
      res.win = den ? Math.round(res.pass / den * 100) : 0;
      return res;
    });

    const avg = Object.fromEntries(tests.map((d, i) =>
      [d, Math.round(stats.filter(s => {
        const r = entries.find(r => (r.provider.id || r.provider) === s.model && r.vars.query == d);
        return r.success;
      }).length / providers.length * 100)]
    ));

    // sort rows by win desc, fail desc, name
    stats.sort((a, b) => b.win - a.win || b.fail - a.fail || a.model.localeCompare(b.model));

    // sort cols by win desc
    tests.sort((a, b) => avg[a] - avg[b]).map(t => t.test);

    // header
    document.getElementById('headerRow').innerHTML =
      ['Model', '%Win', ...tests].map(h => `<th>${h}</th>`).join('');

    // body
    const b = document.getElementById('body');
    stats.forEach(s => {
      const cells = tests.map((d, i) => {
        const r = entries.find(r => (r.provider.id || r.provider) === s.model && r.vars.query == d);
        const o = r.response;
        const st = o?.error ? 'ERROR' : !(o?.output || '').trim() ? 'BLANK' : (r.success ? 'PASS' : 'FAIL');
        const cls = st === 'PASS' ? 'table-success' : st === 'FAIL' ? 'table-danger' : 'table-warning';
        return `<td class="${cls}" data-bs-toggle="popover" data-bs-placement="top" data-bs-content="${r.gradingResult?.reason.replace(/"/g, '&quot;') ?? o.error}">${st}</td>`;
      }).join('');
      b.insertAdjacentHTML('beforeend', `<tr><th>${s.model}</th><td>${s.win}%</td>${cells}</tr>`);
    });

    // Initialize popovers
    bootstrap.Popover.getOrCreateInstance(document.body, { selector: '[data-bs-toggle="popover"]', trigger: 'hover' });

    // footer averages
    const f = document.getElementById('foot');
    f.innerHTML = `<tr class="table-secondary"><th>Average</th><th></th>` +
      tests.map(d => `<th>${avg[d]}%</th>`).join('') + `</tr>`;
  </script>
</body>

</html>
